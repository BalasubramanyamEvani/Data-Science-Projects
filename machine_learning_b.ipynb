{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Model Performance and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Sklearn and Natural Language Processing\n",
    "In this part, you will apply sklearn and related NLP libraries to perform data analysis on the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Before you begin, check that your installed `scikit-learn` version is as specified in `requirements.txt`; otherwise you may not pass the local tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading a subset of the dataset, which contains 5000 movie reviews and their associated sentiment labels (i.e., whether a review is considered positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"imdb_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taran Adarsh a reputed critic praised such a d...</td>\n",
       "      <td>taran adarsh repute critic praise dubba movie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth entertainment value rental especially li...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I liked Antz, but loved \"A Bug's Life\". The an...</td>\n",
       "      <td>like antz love bug life animation put paid def...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This reboot is like a processed McDonald's mea...</td>\n",
       "      <td>reboot like process mcdonald meal compare ang ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The working title was: \"Don't Spank Baby\". &lt;br...</td>\n",
       "      <td>work title spank baby wayne crawford go become...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Taran Adarsh a reputed critic praised such a d...   \n",
       "1  Worth the entertainment value of a rental, esp...   \n",
       "2  I liked Antz, but loved \"A Bug's Life\". The an...   \n",
       "3  This reboot is like a processed McDonald's mea...   \n",
       "4  The working title was: \"Don't Spank Baby\". <br...   \n",
       "\n",
       "                                    processed_review sentiment  \n",
       "0  taran adarsh repute critic praise dubba movie ...  negative  \n",
       "1  worth entertainment value rental especially li...  negative  \n",
       "2  like antz love bug life animation put paid def...  positive  \n",
       "3  reboot like process mcdonald meal compare ang ...  negative  \n",
       "4  work title spank baby wayne crawford go become...  positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review` column contains raw review texts from the original dataset. However, it's always a good idea to process and clean text data before performing analysis. As you have performed this task in Project 3, we will provide the processed reviews for you in this case. The column `processed_review` was constructed by processing and tokenizing the raw reviews, using the `preprocess_text` function from Project 3, and then joining the review tokens by a single space. From this point, you only need to focus on the `processed_review` and `sentiment` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the distribution of class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2500\n",
       "positive    2500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "display(df_reviews['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 2500 positive reviews and 2500 negative reviews. In other words, our dataset is [perfectly balanced](https://i.imgflip.com/303krn.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Count Vectorizer\n",
    "\n",
    "The first feature construction task we will perform is building a term-frequency matrix. Implement the function `count_vectorizer` that uses sklearn's `CountVectorizer` API to construct the term-frequency training matrix and testing matrix, along with the feature names (i.e., the list of words corresponding to the columns in the matrices).\n",
    "\n",
    "One point to keep in mind is that `CountVectorizer` will, by default, do its own preprocessing and tokenization (see the [documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) for more details). As these steps have already performed, we will need to overwrite sklearn's default behaviors by specifying the following parameters:\n",
    "* `analyzer` and `tokenizer` should be `str.split`.\n",
    "* `preprocessor` should be a function that simply returns the input. We have built this function, called `dummy_fun`, for you.\n",
    " \n",
    " \n",
    " **Notes**:\n",
    " * Recall from the data normalization function in Part A that, with any feature construction or transformation task, we will only perform fitting on the train data, and then transform both train and test data. In other words, no fitting should be done on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(x):\n",
    "    return x\n",
    "\n",
    "def count_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the term-frequency matrices for train_data and test_data using CountVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "        \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF matrices\n",
    "    \"\"\"\n",
    "    tf_vectorizer = CountVectorizer(preprocessor=dummy_fun, analyzer=str.split, tokenizer=str.split)\n",
    "    tf_vectorizer.fit(reviews_train)\n",
    "    tf_train = tf_vectorizer.transform(reviews_train)\n",
    "    tf_test = None\n",
    "    if reviews_test is not None:\n",
    "        tf_test = tf_vectorizer.transform(reviews_test)\n",
    "    return tf_train, tf_test, tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_count_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    count_vec_train, count_vec_test, features = count_vectorizer(reviews_train, reviews_test)\n",
    "    assert count_vec_train.shape == (3750, 27242)\n",
    "    assert count_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        count_vec_train.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [70, 65, 168, 77, 139, 132, 28, 139, 453, 89]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        count_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [168, 60, 59, 144, 494, 135, 69, 119, 76, 68]\n",
    "    )\n",
    "    assert features[:10] == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert features[-10:] == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_count_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: TF-IDF Vectorizer\n",
    "\n",
    "Now let's use the TF-IDF feature construction method. Implement the function `tfidf_vectorizer` that uses sklearn's `TfidfVectorizer` API to construct the TF-IDF training matrix and testing matrices, along with the feature names (i.e., the list of words corresponding to the columns in the matrices). Use the same parameter values for `analyzer`, `tokenizer` and `preprocessor` as you did in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF matrices for train_data and test_data using TfidfVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "    \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF-IDF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF-IDF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF-IDF matrices\n",
    "    \"\"\"\n",
    "    tf_idf_vectorizer = TfidfVectorizer(preprocessor=dummy_fun, analyzer=str.split, tokenizer=str.split)\n",
    "    tf_idf_vectorizer.fit(reviews_train)\n",
    "    tf_idf_train = tf_idf_vectorizer.transform(reviews_train)\n",
    "    tf_idf_test = None\n",
    "    if reviews_test is not None:\n",
    "        tf_idf_test = tf_idf_vectorizer.transform(reviews_test)\n",
    "    return tf_idf_train, tf_idf_test, tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_tfidf_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    tfidf_vec_trains, tfidf_vec_test, features = tfidf_vectorizer(reviews_train, reviews_test)\n",
    "    assert tfidf_vec_trains.shape == (3750, 27242)\n",
    "    assert tfidf_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_trains.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.03658925089979, 7.417196035144321, 11.492434722367015, 6.965673648338525, 9.428219597939362, 9.425632229448961, 3.9722806270035345, 9.635230284023372, 11.779155501275017, 7.44670396016231]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.2233277330801196, 4.869804242110142, 6.249091468966529, 9.689812079503804, 11.89432945296538, 9.115185225757216, 6.798492438570971, 8.57464867777901, 7.954528809138947, 6.81383392701789]\n",
    "    )\n",
    "    assert features[:10] == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert features[-10:] == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: Predicting review sentiment\n",
    "Let's now see which feature construction method -- TF or TF-IDF -- is better for predicting review sentiments in our dataset. Our choice of learning algorithm here will be a support vector machine with Gaussian kernel (this means that it uses a different hypothesis function that can also account for non-linearly separable data). You can apply this learning algorithm by creating an instance of sklearn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class, with `kernel = \"rbf\"` and `C = 10`.\n",
    "\n",
    "Implement the function `predict_sentiment` that takes as input the `reviews` and `sentiment` columns of our IMDB dataset and performs the following tasks:\n",
    "1. Convert the `sentiment` column to a vector `y` of 1s and -1s: `positive` corresponds to 1 and `negative` to -1.\n",
    "1. Perform a [stratified k-fold split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) of the review and sentiment vectors, based on the provided `k`. Also set `shuffle` to `True` and `random_state` to the provided `seed`.\n",
    "1. For $f$ from $1 \\to k$:\n",
    "     * Let fold $f$ be the test set, and the remaining $k-1$ folds be the training set.\n",
    "     * Convert the training and testing reviews to feature matrices `X_train` and `X_test`, using either TF or TF-IDF. Which method to use is based on the function parameter `method`.\n",
    "     * Train the SVM model on `X_train, y_train` and evaluate its accuracy $a_f$ on `X_test, y_test`.\n",
    "1. Return $a_1, a_2, \\ldots, a_k$.\n",
    "\n",
    "**Notes**:\n",
    "* As a reminder, accuracy is defined as\n",
    "$$\\text{Acc} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}(y^{(i)} = \\hat y^{(i)}).$$\n",
    "You can also use the `score` function from `SVC` to quickly compute accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review_sentiment(reviews, sentiments, method, k, seed = 0):\n",
    "    \"\"\"\n",
    "    Compute the cross-validated accuracy of SVM with either TF or TF-IDF features\n",
    "    in predicting review sentiment.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "        sentiments (pd.Series[str]) : a Series of movie review sentiments,\n",
    "            containing either \"positive\" or \"negative\"\n",
    "        method (str) : a string which is either \"TF\" or \"TF-IDF\",\n",
    "            specifying which feature construction method to use\n",
    "        k (int) : the number of folds in stratified k-fold split\n",
    "    \n",
    "    kwargs:\n",
    "        seed (int) : the random generator seed for kfold split\n",
    "        \n",
    "    return:\n",
    "        List[float] : a list of k accuracy values from evaluating a trained SVM model\n",
    "            on each of the k folds, using the remaining folds as training data\n",
    "    \"\"\"\n",
    "    clf = SVC(kernel=\"rbf\", C=10)\n",
    "    Y = sentiments.to_numpy()\n",
    "    X = reviews.to_numpy()\n",
    "    Y = np.where(Y == \"positive\", 1, -1)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    skf.get_n_splits(X, Y)\n",
    "    accuracies = []\n",
    "    vectorizer = tfidf_vectorizer if method == \"TF-IDF\" else count_vectorizer\n",
    "    for train_indexes, test_indexes in skf.split(X, Y):\n",
    "        X_train, Y_train = X[train_indexes], Y[train_indexes]\n",
    "        X_test, Y_test = X[test_indexes], Y[test_indexes]\n",
    "        X_train, X_test, _ = vectorizer(X_train, X_test)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        acc = clf.score(X_test, Y_test)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.878, 0.836, 0.854, 0.824, 0.826, 0.824, 0.824, 0.85, 0.844, 0.83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:493: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'preprocessor' will not be used\"\n",
      "/home/bala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:509: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "Cross-validated accuracy of SVM with TF matrices 0.8389999999999999\n",
      "Cross-validated accuracy of SVM with TF-IDF matrices 0.8617999999999999\n"
     ]
    }
   ],
   "source": [
    "def test_predict_review_sentiment():\n",
    "    # prediction based on TF\n",
    "    count_vec_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF\", 10)\n",
    "    assert count_vec_accs == [0.878, 0.836, 0.854, 0.824, 0.826, 0.824, 0.824, 0.85, 0.844, 0.83]\n",
    "    \n",
    "    # prediction based on TF-IDF\n",
    "    tf_idf_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF-IDF\", 10)\n",
    "    assert tf_idf_accs == [0.88, 0.862, 0.85, 0.868, 0.854, 0.846, 0.864, 0.874, 0.874, 0.846]\n",
    "    print(\"All tests passed!\")\n",
    "    print(\"Cross-validated accuracy of SVM with TF matrices\", np.mean(count_vec_accs))\n",
    "    print(\"Cross-validated accuracy of SVM with TF-IDF matrices\", np.mean(tf_idf_accs))\n",
    "    \n",
    "test_predict_review_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using TF-IDF features yields better cross-validated accuracy than using TF features (when the learning algorithm is SVM with RBF kernel and $C = 10$), although the difference in this case is not large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: Topic modeling and word distribution\n",
    "Let's now try to understand the review texts a bit more. We can treat all the reviews as a corpus and perform Latent Dirichlet Allocation to extract the corpus topics. We can also see which words are most frequent in a given topic. Implement the function `top_words_by_topic` that takes as input the `processed_reviews` column in our IMDB dataset and performs the following tasks:\n",
    "\n",
    "1. Build a term-frequency matrix out of this column. Remember to use the same `CountVectorizer` parameters as in Q11.\n",
    "1. Input this matrix to sklearn's `LatentDirichletAllocation`. The number of topics and random generator seed are provided as function parameters. You should specify `learning_method` as `\"online\"`.\n",
    "1. In the resulting word-topic matrix, identify the most frequent `n_top_words` in each topic. These most frequent words should be sorted from lower to higher frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_by_topic(reviews, n_topics = 10, n_top_words = 20, seed = 0):\n",
    "    \"\"\"\n",
    "    Perform topic modeling on the movie review corpus and return the most frequent words in each topic.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "    \n",
    "    kwargs:\n",
    "        n_topics (int) : the number of topics to model by LDA\n",
    "        n_top_words (int) : the number of most frequent words to identify in each topic\n",
    "        seed (int) : the random generator seed for LDA\n",
    "    \n",
    "    return:\n",
    "        List[List[str]] : a nested list of words, where each of the n_topics inner lists\n",
    "            contains the n_top_words most frequent words in a given topic\n",
    "    \"\"\"\n",
    "    tf, _, features = count_vectorizer(reviews)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, learning_method=\"online\", random_state=seed)\n",
    "    lda.fit(tf)\n",
    "    res = []\n",
    "    for topic, distribution in enumerate(lda.components_):\n",
    "        feature_indexes = np.argsort(distribution)[::-1][:n_top_words]\n",
    "        top_features = [features[index] for index in feature_indexes][::-1]\n",
    "        res.append(top_features)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_top_words_by_topic():\n",
    "    corpus = pd.Series([\n",
    "        \"I like to eat broccoli and bananas\",\n",
    "        \"I ate a banana and spinach smoothie for breakfast\",\n",
    "        \"Chinchillas and kittens are cute\",\n",
    "        \"My sister adopted a kitten yesterday\",\n",
    "        \"Look at this cute hamster munching on a piece of broccoli\"\n",
    "    ])\n",
    "    top_words = top_words_by_topic(corpus, n_topics = 2, n_top_words = 5)\n",
    "    assert top_words == [['Look', 'broccoli', 'and', 'cute', 'a'], ['I', 'eat', 'like', 'to', 'and']]\n",
    "    \n",
    "    top_words = top_words_by_topic(df_reviews[\"processed_review\"], n_topics = 5, n_top_words = 5)\n",
    "    assert top_words == [\n",
    "        ['performance', 'play', 'version', 'jack', 'role'],\n",
    "        ['dancer', 'paris', 'dance', 'cartoon', 'hitchcock'],\n",
    "        ['make', 'like', 'one', 'film', 'movie'],\n",
    "        ['film', 'father', 'world', 'american', 'war'],\n",
    "        ['mad', 'sheriff', 'match', 'carmen', 'arthur']\n",
    "    ]\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_top_words_by_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Word embedding and word similarity\n",
    "Finally, let's look at how we can train a word embedding model from our movie review corpus. Unfortunately, gensim's `Word2Vec` does not output reproducible results across different environments, so we will not grade this question. Instead, here we provide the implementation of the function `get_most_similar_words` that takes as input the `processed_reviews` column in our IMDB dataset, and for each input word, returns a list of `n_similar_words` top similar tokens to that word, based on the Word2Vec model. Here the tokens are ordered from lower to higher similarity values.\n",
    "\n",
    "You can see the code below and play around with different settings to better understand Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_words(reviews, input_words, n_similar_words):\n",
    "    corpus = [review.split() for review in reviews]\n",
    "    model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, workers = 4, min_count = 1)\n",
    "    similar_words = []\n",
    "    for inp_word in input_words:\n",
    "        similar_words.append([w for w in sorted(model.wv.most_similar(inp_word, topn = n_similar_words), key = lambda x: x[1])])\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_find_most_similar_words():\n",
    "    input_words = [\"see\", \"good\", \"bad\", \"watch\", \"check\"]\n",
    "    most_similar_words = find_most_similar_words(df_reviews[\"processed_review\"], input_words, 7)\n",
    "    for i in range(len(input_words)):\n",
    "        print(f\"Words most similar to '{input_words[i]}':\")\n",
    "        print(most_similar_words[i])\n",
    "        print()\n",
    "    \n",
    "test_find_most_similar_words()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
